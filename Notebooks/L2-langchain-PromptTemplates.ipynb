{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db97323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1008dbf5",
   "metadata": {},
   "source": [
    "#### **Static Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1ac60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich wurde von Google trainiert und spreche viele Sprachen. Ich kann Texte auf Deutsch verstehen und generieren.  Ich kann auch in anderen Sprachen kommunizieren, aber meine Fähigkeiten variieren je nach Sprache.  Meine Stärke liegt in der Verarbeitung und Generierung von Texten, nicht im direkten Sprechen.\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\"Welche sprachen sprichst du?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4968667a",
   "metadata": {},
   "source": [
    "#### **Dynamic Prompt Template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46952bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a {profession}, who only talks about related subject.\n",
    "    If someone asks you a question that is not related to your profession, you will answer with \"I don't know\".\n",
    "    \n",
    "    Answer the following question:\n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"profession\", \"question\"]\n",
    ")\n",
    "\n",
    "prompt_template = prompt.invoke(\n",
    "    {\n",
    "        \"profession\": \"doctor\",\n",
    "        \"question\": \"What is the capital of France?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response = model.invoke(prompt_template)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e221e",
   "metadata": {},
   "source": [
    "##### **Save and Load Template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b762e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.save('template.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4f883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "template = load_prompt('template.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2522f8c",
   "metadata": {},
   "source": [
    "#### **Chat Prompt Template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e79b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates offer structure and control for generating consistent outputs from large language models, while chat prompt templates add conversational context.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    ('system', \"You are a {profession}, who only talks about related subject and says I don't know to non-related queries\"),\n",
    "    ('human', \"{query}\")\n",
    "])\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        'profession':'data-scientist', \n",
    "        'query':'when we use prompt template and chat prompt template just give oneliner'\n",
    "    }\n",
    ")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627aa43a",
   "metadata": {},
   "source": [
    "#### **Message Placeholder**\n",
    "\n",
    "- Use for loading the chat history as LLM context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "def load_chats():\n",
    "    \"\"\"create custom function for loading the chats from database\"\"\"\n",
    "    pass\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    ('system', 'you are an e-commerce chatbot'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('human', '{query}')\n",
    "])\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    if user_query.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    prompt = template.invoke(\n",
    "        {\n",
    "            'chat_history':chat_history,\n",
    "            'query': user_query\n",
    "        }\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    # store chats using message-labels\n",
    "    chat_history.append(HumanMessage(content=user_query))\n",
    "    chat_history.append(AIMessage(content=response.content))\n",
    "    \n",
    "    print(\"AI: \", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
